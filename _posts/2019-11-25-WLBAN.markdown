---
layout: article
title:  "细粒度识别论文阅读《Weakly Supervised Bilinear Attention Network for Fine-Grained Visual Classification》"
date:   2019-11-21 09:53:48 +0000
tags: Fine-grained
mathjax: true
mathjax_autoNumber: true
---



## 任务


细粒度识别，弱监督信息下识别子类分类问题，例如分辨不同鸟、车、飞机。  



## 出发点

<!-- 细粒度的识别难点在于网络很难从一个固定分辨率大小的图中自适应的找到合适分类的纹理特征，该论文通过放大图像中的纹理特征以期望获得网络的关注（在分类上更有权重）。 -->

细粒度识别领域常常采用弱监督信息进行目标定位，然而误检和遮挡会导致目标定位发生较大的偏差，一定程度上引起推断的波动性（即学习过程不稳定）和损害准确率的增长。
作者从attention角度出发，对特征图进行attention量化，实现另一种形式的目标定位，two-stage的方式进一步提取裁剪图片的特征进行联合分类。







## 难点

1. attention图如何产生？
2. 特征图和attention图搭配使用？
3. 如何使用attention图进行裁剪？  
<!-- 3. 如何？ -->


## 方法

### 网络总览

1. 原图经过InceptionV3特征提取产生特征图，其中使用1*1卷积产生attention图(128维度)，将attention图和特征图进行bilinear汇合，称之为bilinear attention pooling（BAP）。最后进行提取出相应的特征。
2. attention图进行通道间的平均加权，使用论文[《Learning deep features for discriminative localization》](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf)
中的方法进行裁剪，得到目标裁剪图。
3. 裁剪图如1中的过程得到特征，最后与原图特征联合分类。


<figure>
<a><img src="{{site.url}}/assert/wsban_all.png"></a>
</figure>
<!-- As an analogy [15] to natural language processing, shuffling
words in a sentence would force the neural network to focus
on discriminative words and neglect irrelevant ones. Similarly, if local regions in an image are “shuffled”, the neural
network would be forced to learn from discriminative region details for classification. -->

**Bilinear Attention Pooling**

<!-- ![navigate](assert/navigate.png) -->

就是一般的bilinear pooling，两流的输入，一流为特征图，另一流为attention图。

设 $$ X = N * H * W  -> reshape -> X = N * HW $$, $$ A = M * H * W -> reshape -> A = M * HW $$，
则bilinear输出定义为：

$$ B = XA^T $$



所以得到$$B$$的shape为$$(N,M)$$。

<!-- 
$$ \ell(x, y) = L = \{l_1,\dots,l_N\}^\top , \quad$$

$$l_n = \left( x_n - y_n \right)^2 $$ -->


<!-- $$a=\frac{1}{1+sin(x)}$$ -->

<!-- ![](http://latex.codecogs.com/gif.latex?\\a=\frac{1}{1+sin(x)}) -->



**Attention Cropping**

从InceptionV3的某一层获取卷积层输出，使用1*1卷积获取attention图。

将attention按照通道维度平均池化，获得总的全局attention图。根据全局attention图裁剪得到目标图。

裁剪的方法参见论文[《Learning deep features for discriminative localization》](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf)


裁剪的理解：attention图的按照最大连通域裁剪

做法，取最大值的20%为阈值，计算最大连通域，裁剪最大连通域的矩形框。

参考做法


    import cv2
    import os
    import numpy as np

    from skimage.morphology import label
    from skimage.measure import regionprops


    img_path = '/root/skb/inception_bili/images.jpg'

    img = cv2.imread(img_path,0)

    print(img.max())



    max_value = np.max(img)




    ret, ee = cv2.threshold(img, 0.8*max_value, max_value, cv2.THRESH_BINARY)

    cv2.imwrite('/root/skb/inception_bili/ee.png',ee)

    labeled_img, num = label(ee, neighbors=4, background=0, return_num=True)

    # minr, minc, maxr, maxc = regionprops(labeled_img)[max_label].bbox

    max_area = -1
    for region in regionprops(labeled_img):
        # skip small images
        if region.area < 50:
            continue
        if region.area > max_area:
        #print(regionprops(labeled_img)[max_label])
            max_area = region.area
            minr, minc, maxr, maxc = region.bbox

    crop_img = img[minr:maxr,minc:maxc]
    cv2.imwrite('/root/skb/inception_bili/crop.png',crop_img)

    print(ret,ee)




<!-- <img class="image image--lg" src="{{site.url}}/assert/tasn_ab.png"/> -->

**loss设计**

两阶段的图像分类loss和attention图的中心loss

center loss 理解，
对每一个类别分别有个M个part中心，期望网络能够靠近数据中心

<!-- $$ d_{H \Delta H} $$ -->

## **RESULT**

<!-- ![result](assert/result.png) -->

<figure>
<a><img src="{{site.url}}/assert/wsban_result.png"></a>
</figure>



[原文链接《Weakly Supervised Bilinear Attention Network for Fine-Grained Visual Classification》](https://arxiv.org/pdf/1808.02152.pdf)

[原文链接《See Better Before Looking Closer: Weakly Supervised Data Augmentation Network for Fine-Grained Visual Classification》](https://arxiv.org/pdf/1901.09891.pdf)
<!-- https://arxiv.org/pdf/1901.09891.pdf -->
